{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we test out BERT on CoQA ideas:\n",
    "\n",
    "BERT on COQA will be mostly based on BERT on Squad.  We will adapt Google implementation of BERT on Squad.  Suqad is a single turn QA dataset, but CoQA is a multiturn QA dataset with abstractive answer to questions unlike Squad which is extractive answers. But we will adapt BERT on Squad implementation to our purpose. The idea is to find the span text first and then abstract the answer from it.  We will use story text plus QA history prior to current question as the input.  How to do incorporate the QA history?  since the input is limited in size, up to 512 tokens (limitation of BERT model),  we will have to use sliding window technique for story text and limit our QA history to the last three turns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 17:39:42.983809 4321366912 deprecation_wrapper.py:119] From /Users/wweschen/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "## Required parameters\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", \"pretrained/uncased_L-12_H-768_A-12/bert_config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", 'pretrained/uncased_L-12_H-768_A-12/vocab.txt',\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", 'tmp/coqa/',\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "flags.DEFINE_string(\"train_file\", 'datasets/COQA/coqa-train-v1.0.json',\n",
    "                    \"CoQA json for training.  \")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", None,\n",
    "    \"CoQA json for predictions. \")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", 'pretrained/uncased_L-12_H-768_A-12/bert_model.ckpt',\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 512,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 6, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_best_size\", 20,\n",
    "    \"The total number of n-best predictions to generate in the \"\n",
    "    \"nbest_predictions.json output file.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"verbose_logging\", False,\n",
    "    \"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal CoQA evaluation.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"null_score_diff_threshold\", 0.0,\n",
    "    \"If null_score - best_non_null is greater than the threshold predict null.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoqaExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\n",
    "\n",
    "     For examples without an answer, the start and end position are -1.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               qas_id,\n",
    "               turn_id,\n",
    "               question_text,\n",
    "               doc_tokens,\n",
    "               gold_answer_text=None,\n",
    "               orig_answer_text=None,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               qa_history_text = None):\n",
    "    self.qas_id = qas_id\n",
    "    self.turn_id=turn_id\n",
    "    self.question_text = question_text\n",
    "    self.doc_tokens = doc_tokens\n",
    "    self.gold_answer_text=gold_answer_text\n",
    "    self.orig_answer_text = orig_answer_text\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position \n",
    "    self.qa_history_text = qa_history_text\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.__repr__()\n",
    "\n",
    "  def __repr__(self):\n",
    "    s = \"\"\n",
    "    s += \"\\nqas_id: %s\" % (tokenization.printable_text(self.qas_id))\n",
    "    s +=\"\\nturn id: %s\" % (self.turn_id)\n",
    "    s += \"\\nquestion_text: %s\" % (\n",
    "        tokenization.printable_text(self.question_text))\n",
    "    s +=\"\\ngold_answer_text: %s\" % (self.gold_answer_text)\n",
    "    s +=\"\\noriginal text: %s\" % (self.orig_answer_text)\n",
    "    s += \"\\ndoc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "    s += \"\\nlast_three_qa: [%s]\" %  (self.qa_history_text)\n",
    "    if self.start_position:\n",
    "      s += \"\\nstart_position: %d\" % (self.start_position)\n",
    "    if self.start_position:\n",
    "      s += \"\\nend_position: %d\" % (self.end_position) \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_coqa_examples(input_file, is_training):\n",
    "  \"\"\"Read a CoQA json file into a list of SquadExample.\"\"\"\n",
    "  with tf.gfile.Open(input_file, \"r\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "  def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "  examples = [] \n",
    "   \n",
    "  #for entry in input_data:\n",
    "  #print(len(input_data))\n",
    "  for n in range(100):\n",
    "    entry=input_data[n] \n",
    "    paragraph_text = entry[\"story\"]\n",
    "    qas_id =entry[\"id\"]\n",
    "    #print(qas_id)\n",
    "    doc_tokens = []\n",
    "    char_to_word_offset = []\n",
    "    prev_is_whitespace = True\n",
    "    \n",
    "    \n",
    "    for c in paragraph_text:\n",
    "        if is_whitespace(c):\n",
    "              prev_is_whitespace = True\n",
    "        else:\n",
    "              if prev_is_whitespace:\n",
    "                    doc_tokens.append(c)\n",
    "              else:\n",
    "                    doc_tokens[-1] += c\n",
    "              prev_is_whitespace = False\n",
    "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "    \n",
    "    history_qas=[]\n",
    "    for i in range(len(entry[\"questions\"])):\n",
    "        que= entry[\"questions\"][i]\n",
    "        turn_id = int(que[\"turn_id\"])\n",
    "        question_text = que[\"input_text\"]\n",
    "        #print('turn id:',turn_id)\n",
    "        answer = entry['answers'][i]\n",
    "        start_position = None\n",
    "        end_position = None\n",
    "        orig_answer_text = \"\" \n",
    "        qa_history_text= \"\"\n",
    "        \n",
    "        \n",
    "        orig_answer_text = answer[\"span_text\"] \n",
    "        gold_answer_text = answer[\"input_text\"] \n",
    "\n",
    "        answer_offset = answer[\"span_start\"]\n",
    "        answer_length = len(orig_answer_text)\n",
    "\n",
    "        start_position = char_to_word_offset[answer_offset]\n",
    "        end_position = char_to_word_offset[answer_offset + answer_length -1] \n",
    "\n",
    "        history_qas.append('{} {}.'.format(question_text,gold_answer_text))\n",
    "        #print(len(history_qas)) \n",
    "        if i==1:\n",
    "              qa_history_text=  history_qas[0]\n",
    "        if i==2:\n",
    "            qa_history_text= '{} {}'.format(history_qas[0],history_qas[1])\n",
    "        if i>2:\n",
    "            qa_history_text= '{} {} {}'.format(history_qas[i-3],history_qas[i-2],history_qas[i-1])\n",
    "            #print(qa_history_text)\n",
    "        if not is_training:\n",
    "            start_position = -1\n",
    "            end_position = -1\n",
    "            orig_answer_text = \"\"\n",
    "            gold_answer_text=\"\"\n",
    "            \n",
    "        example = CoqaExample(\n",
    "            qas_id=qas_id,\n",
    "            turn_id=turn_id,\n",
    "            question_text=question_text,\n",
    "            doc_tokens=doc_tokens,\n",
    "            gold_answer_text=gold_answer_text,\n",
    "            orig_answer_text=orig_answer_text,\n",
    "            start_position=start_position,\n",
    "            end_position=end_position, \n",
    "            qa_history_text=qa_history_text\n",
    "        )\n",
    "        examples.append(example)\n",
    " \n",
    "  return examples\n",
    "\n",
    "examples=read_coqa_examples('datasets/COQA/coqa-dev-v1.0.json',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " qas_id: 3dr23u6we5exclen4th8uq9rb42tel\n",
       " turn id: 1\n",
       " question_text: What color was Cotton?\n",
       " gold_answer_text: white\n",
       " original text: a little white kitten named Cotton\n",
       " doc_tokens: [Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \"What are you doing, Cotton?!\" \"I only wanted to be more like you\". Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" Then Cotton thought, \"I change my mind. I like being special\".]\n",
       " last_three_qa: []\n",
       " start_position: 13\n",
       " end_position: 18, \n",
       " qas_id: 3dr23u6we5exclen4th8uq9rb42tel\n",
       " turn id: 2\n",
       " question_text: Where did she live?\n",
       " gold_answer_text: in a barn\n",
       " original text: in a barn near a farm house, there lived a little white kitten\n",
       " doc_tokens: [Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \"What are you doing, Cotton?!\" \"I only wanted to be more like you\". Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" Then Cotton thought, \"I change my mind. I like being special\".]\n",
       " last_three_qa: [What color was Cotton? white.]\n",
       " start_position: 4\n",
       " end_position: 16]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens=[]\n",
    "for i in range(len(examples)):\n",
    "    lens.append(len('{}'.format(examples[i].qa_history_text)))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    108647.000000\n",
       "mean        116.980045\n",
       "std          54.912138\n",
       "min           4.000000\n",
       "25%          90.000000\n",
       "50%         117.000000\n",
       "75%         147.000000\n",
       "max        1928.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(lens).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3231"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open(\"pretrained/uncased_L-12_H-768_A-12/vocab.txt\", \"r\")\n",
    "if f.mode == 'r':\n",
    "    contents =f.read()\n",
    "    vocab=contents.split()\n",
    "    \n",
    "vocab[5022]\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "  vocab = collections.OrderedDict()\n",
    "  index = 0\n",
    "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "    while True:\n",
    "      token = convert_to_unicode(reader.readline())\n",
    "      if not token:\n",
    "        break\n",
    "      token = token.strip()\n",
    "      vocab[token] = index\n",
    "      index += 1\n",
    "  return vocab\n",
    "def convert_to_unicode(text):\n",
    "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    elif isinstance(text, unicode):\n",
    "      return text\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "vocab= load_vocab(\"pretrained/uncased_L-12_H-768_A-12/vocab.txt\")\n",
    "\n",
    "vocab['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'hidden_dim' is defined twice. First from /Users/wweschen/venv/lib/python3.6/site-packages/ipykernel_launcher.py, Second from /Users/wweschen/venv/lib/python3.6/site-packages/ipykernel_launcher.py.  Description from first occurrence: dimension of RNN hidden states",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-45889247249a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hidden_dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dimension of RNN hidden states'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emb_dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dimension of word embeddings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'minibatch size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[0;34m(name, default, help, lower_bound, upper_bound, flag_values, **args)\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegerParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m   \u001b[0m_register_bounds_validator_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'hidden_dim' is defined twice. First from /Users/wweschen/venv/lib/python3.6/site-packages/ipykernel_launcher.py, Second from /Users/wweschen/venv/lib/python3.6/site-packages/ipykernel_launcher.py.  Description from first occurrence: dimension of RNN hidden states"
     ]
    }
   ],
   "source": [
    " \n",
    "############ PGNet settings#########\n",
    "\n",
    "# Where to find data\n",
    "#tf.app.flags.DEFINE_string('data_path', '', 'Path expression to tf.Example datafiles. Can include wildcards to access multiple datafiles.')\n",
    "#tf.app.flags.DEFINE_string('vocab_path', '', 'Path expression to text vocabulary file.')\n",
    "\n",
    "# Important settings\n",
    "#tf.app.flags.DEFINE_string('mode', 'train', 'must be one of train/eval/decode')\n",
    "#tf.app.flags.DEFINE_boolean('single_pass', False, 'For decode mode only. If True, run eval on the full dataset using a fixed checkpoint, i.e. take the current checkpoint, and use it to produce one summary for each example in the dataset, write the summaries to file and then get ROUGE scores for the whole dataset. If False (default), run concurrent decoding, i.e. repeatedly load latest checkpoint, use it to produce summaries for randomly-chosen examples and log the results to screen, indefinitely.')\n",
    "\n",
    "# Where to save output\n",
    "#tf.app.flags.DEFINE_string('log_root', '', 'Root directory for all logging.')\n",
    "#tf.app.flags.DEFINE_string('exp_name', '', 'Name for experiment. Logs will be saved in a directory with this name, under log_root.')\n",
    "\n",
    "# Hyperparameters\n",
    "tf.app.flags.DEFINE_integer('hidden_dim', 256, 'dimension of RNN hidden states')\n",
    "tf.app.flags.DEFINE_integer('emb_dim', 128, 'dimension of word embeddings')\n",
    "tf.app.flags.DEFINE_integer('batch_size', 16, 'minibatch size')\n",
    "tf.app.flags.DEFINE_integer('max_enc_steps', 400, 'max timesteps of encoder (max source text tokens)')\n",
    "tf.app.flags.DEFINE_integer('max_dec_steps', 100, 'max timesteps of decoder (max summary tokens)')\n",
    "tf.app.flags.DEFINE_integer('beam_size', 4, 'beam size for beam search decoding.')\n",
    "tf.app.flags.DEFINE_integer('min_dec_steps', 35, 'Minimum sequence length of generated summary. Applies only for beam search decoding mode')\n",
    "tf.app.flags.DEFINE_integer('vocab_size', 50000, 'Size of vocabulary. These will be read from the vocabulary file in order. If the vocabulary file contains fewer words than this number, or if this number is set to 0, will take all words in the vocabulary file.')\n",
    "tf.app.flags.DEFINE_float('lr', 0.15, 'learning rate')\n",
    "tf.app.flags.DEFINE_float('adagrad_init_acc', 0.1, 'initial accumulator value for Adagrad')\n",
    "tf.app.flags.DEFINE_float('rand_unif_init_mag', 0.02, 'magnitude for lstm cells random uniform inititalization')\n",
    "tf.app.flags.DEFINE_float('trunc_norm_init_std', 1e-4, 'std of trunc norm init, used for initializing everything else')\n",
    "tf.app.flags.DEFINE_float('max_grad_norm', 2.0, 'for gradient clipping')\n",
    "\n",
    "# Pointer-generator or baseline model\n",
    "tf.app.flags.DEFINE_boolean('pointer_gen', True, 'If True, use pointer-generator model. If False, use baseline model.')\n",
    "\n",
    "# Coverage hyperparameters\n",
    "tf.app.flags.DEFINE_boolean('coverage', True, 'Use coverage mechanism. Note, the experiments reported in the ACL paper train WITHOUT coverage until converged, and then train for a short phase WITH coverage afterwards. i.e. to reproduce the results in the ACL paper, turn this off for most of training then turn on for a short phase at the end.')\n",
    "tf.app.flags.DEFINE_float('cov_loss_wt', 1.0, 'Weight of coverage loss (lambda in the paper). If zero, then no incentive to minimize coverage loss.')\n",
    "\n",
    "# Utility flags, for restoring and changing checkpoints\n",
    "tf.app.flags.DEFINE_boolean('convert_to_coverage_model', False, 'Convert a non-coverage model to a coverage model. Turn this on and run in train mode. Your current training model will be copied to a new version (same name with _cov_init appended) that will be ready to run with coverage flag turned on, for the coverage training stage.')\n",
    "tf.app.flags.DEFINE_boolean('restore_best_model', False, 'Restore the best model in the eval/ dir and save it in the train/ dir, ready to be used for further training. Useful for early stopping, or if your training checkpoint has become corrupted with e.g. NaN values.')\n",
    "\n",
    "# Debugging. See https://www.tensorflow.org/programmers_guide/debugger\n",
    "tf.app.flags.DEFINE_boolean('debug', False, \"Run in tensorflow's debug mode (watches for NaN/inf values)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logtostderr False\n",
      "alsologtostderr False\n",
      "log_dir \n",
      "v -1\n",
      "verbosity -1\n",
      "stderrthreshold fatal\n",
      "showprefixforinfo True\n",
      "run_with_pdb False\n",
      "pdb_post_mortem False\n",
      "run_with_profiling False\n",
      "profile_file None\n",
      "use_cprofile_for_profiling True\n",
      "only_check_args False\n",
      "op_conversion_fallback_to_while_loop False\n",
      "test_random_seed 301\n",
      "test_srcdir \n",
      "test_tmpdir /var/folders/tv/6j6nb_mx2876v_t3l8r4ck0w0000gn/T/absl_testing\n",
      "test_randomize_ordering_seed None\n",
      "xml_output_file \n",
      "bert_config_file pretrained/uncased_L-12_H-768_A-12/bert_config.json\n",
      "vocab_file pretrained/uncased_L-12_H-768_A-12/vocab.txt\n",
      "output_dir tmp/coqa/\n",
      "train_file datasets/COQA/coqa-train-v1.0.json\n",
      "predict_file None\n",
      "init_checkpoint pretrained/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "do_lower_case True\n",
      "max_seq_length 512\n",
      "doc_stride 128\n",
      "max_query_length 64\n",
      "do_train True\n",
      "do_predict True\n",
      "train_batch_size 6\n",
      "predict_batch_size 8\n",
      "learning_rate 5e-05\n",
      "num_train_epochs 3.0\n",
      "warmup_proportion 0.1\n",
      "save_checkpoints_steps 1000\n",
      "iterations_per_loop 1000\n",
      "n_best_size 20\n",
      "max_answer_length 30\n",
      "use_tpu False\n",
      "tpu_name None\n",
      "tpu_zone None\n",
      "gcp_project None\n",
      "master None\n",
      "num_tpu_cores 8\n",
      "verbose_logging False\n",
      "null_score_diff_threshold 0.0\n",
      "hidden_dim 256\n",
      "emb_dim 128\n",
      "batch_size 16\n",
      "max_enc_steps 400\n",
      "max_dec_steps 100\n",
      "beam_size 4\n",
      "min_dec_steps 35\n",
      "vocab_size 50000\n",
      "lr 0.15\n",
      "adagrad_init_acc 0.1\n",
      "rand_unif_init_mag 0.02\n",
      "trunc_norm_init_std 0.0001\n",
      "max_grad_norm 2.0\n",
      "pointer_gen True\n",
      "coverage True\n",
      "cov_loss_wt 1.0\n",
      "convert_to_coverage_model False\n",
      "restore_best_model False\n",
      "debug False\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bert_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-9e165c5b2917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mhps_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m  \u001b[0;31m# add it to the dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhps_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mhps_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a namedtuple hps, containing the values of the hyperparameters that the model needs\n",
    "hparam_list = ['mode', 'lr', 'adagrad_init_acc', 'rand_unif_init_mag', 'trunc_norm_init_std', 'max_grad_norm',\n",
    "             'hidden_dim', 'emb_dim', 'batch_size', 'max_dec_steps', 'max_enc_steps', 'coverage', 'cov_loss_wt',\n",
    "             'pointer_gen']\n",
    "hps_dict = {}\n",
    "for key, val in FLAGS.__flags.items():  # for each flag\n",
    "  print(key,val.value)\n",
    "  if key in hparam_list:  # if it's in the lis\n",
    "      hps_dict[key] = val.  # add it to the dict\n",
    "\n",
    "hps_dict['vocab_size'] = bert_config.vocab_size\n",
    "if is_training:\n",
    "  hps_dict['mode'] = 'train'\n",
    "else:\n",
    "  hps_dict['mode']='decode'\n",
    "\n",
    "hps = collections.namedtuple(\"HParams\", hps_dict.keys())(**hps_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('logtostderr', <absl.flags._flag.BooleanFlag object at 0x126de1c50>), ('alsologtostderr', <absl.flags._flag.BooleanFlag object at 0x126dea0f0>), ('log_dir', <absl.flags._flag.Flag object at 0x126dea198>), ('v', <absl.logging._VerbosityFlag object at 0x126dea208>), ('verbosity', <absl.logging._VerbosityFlag object at 0x126dea208>), ('stderrthreshold', <absl.logging._StderrthresholdFlag object at 0x126dea2e8>), ('showprefixforinfo', <absl.flags._flag.BooleanFlag object at 0x126dea400>), ('run_with_pdb', <absl.flags._flag.BooleanFlag object at 0x126de12e8>), ('pdb_post_mortem', <absl.flags._flag.BooleanFlag object at 0x126d806d8>), ('run_with_profiling', <absl.flags._flag.BooleanFlag object at 0x126d80748>), ('profile_file', <absl.flags._flag.Flag object at 0x126dea860>), ('use_cprofile_for_profiling', <absl.flags._flag.BooleanFlag object at 0x126dea898>), ('only_check_args', <absl.flags._flag.BooleanFlag object at 0x126dea908>), ('op_conversion_fallback_to_while_loop', <absl.flags._flag.BooleanFlag object at 0x128667668>), ('test_random_seed', <absl.flags._flag.Flag object at 0x1331214e0>), ('test_srcdir', <absl.flags._flag.Flag object at 0x133121630>), ('test_tmpdir', <absl.flags._flag.Flag object at 0x133121828>), ('test_randomize_ordering_seed', <absl.flags._flag.Flag object at 0x133129550>), ('xml_output_file', <absl.flags._flag.Flag object at 0x133129e80>), ('bert_config_file', <absl.flags._flag.Flag object at 0x10e0d0c18>), ('vocab_file', <absl.flags._flag.Flag object at 0x10e0d0c50>), ('output_dir', <absl.flags._flag.Flag object at 0x134da08d0>), ('train_file', <absl.flags._flag.Flag object at 0x134da0a20>), ('predict_file', <absl.flags._flag.Flag object at 0x134da0a90>), ('init_checkpoint', <absl.flags._flag.Flag object at 0x134da0b00>), ('do_lower_case', <absl.flags._flag.BooleanFlag object at 0x134da0c18>), ('max_seq_length', <absl.flags._flag.Flag object at 0x134da0be0>), ('doc_stride', <absl.flags._flag.Flag object at 0x134da0ba8>), ('max_query_length', <absl.flags._flag.Flag object at 0x134da0c50>), ('do_train', <absl.flags._flag.BooleanFlag object at 0x134da0e10>), ('do_predict', <absl.flags._flag.BooleanFlag object at 0x134da0eb8>), ('train_batch_size', <absl.flags._flag.Flag object at 0x134da0e80>), ('predict_batch_size', <absl.flags._flag.Flag object at 0x134da0ef0>), ('learning_rate', <absl.flags._flag.Flag object at 0x134db60f0>), ('num_train_epochs', <absl.flags._flag.Flag object at 0x134db6048>), ('warmup_proportion', <absl.flags._flag.Flag object at 0x134db6160>), ('save_checkpoints_steps', <absl.flags._flag.Flag object at 0x134db61d0>), ('iterations_per_loop', <absl.flags._flag.Flag object at 0x134db6278>), ('n_best_size', <absl.flags._flag.Flag object at 0x134db6320>), ('max_answer_length', <absl.flags._flag.Flag object at 0x134db63c8>), ('use_tpu', <absl.flags._flag.BooleanFlag object at 0x134db6550>), ('tpu_name', <absl.flags._flag.Flag object at 0x134db6518>), ('tpu_zone', <absl.flags._flag.Flag object at 0x134db65c0>), ('gcp_project', <absl.flags._flag.Flag object at 0x134db6668>), ('master', <absl.flags._flag.Flag object at 0x134db66d8>), ('num_tpu_cores', <absl.flags._flag.Flag object at 0x134db6748>), ('verbose_logging', <absl.flags._flag.BooleanFlag object at 0x134db6828>), ('null_score_diff_threshold', <absl.flags._flag.Flag object at 0x134db67f0>), ('hidden_dim', <absl.flags._flag.Flag object at 0x13b9c6dd8>), ('emb_dim', <absl.flags._flag.Flag object at 0x13b9c6eb8>), ('batch_size', <absl.flags._flag.Flag object at 0x13b9c6e10>), ('max_enc_steps', <absl.flags._flag.Flag object at 0x13b9c6cf8>), ('max_dec_steps', <absl.flags._flag.Flag object at 0x13b9db128>), ('beam_size', <absl.flags._flag.Flag object at 0x13b9db0f0>), ('min_dec_steps', <absl.flags._flag.Flag object at 0x13b9db160>), ('vocab_size', <absl.flags._flag.Flag object at 0x13b9db320>), ('lr', <absl.flags._flag.Flag object at 0x13b9db240>), ('adagrad_init_acc', <absl.flags._flag.Flag object at 0x13b9db390>), ('rand_unif_init_mag', <absl.flags._flag.Flag object at 0x13b9db588>), ('trunc_norm_init_std', <absl.flags._flag.Flag object at 0x13b9db518>), ('max_grad_norm', <absl.flags._flag.Flag object at 0x13b9db7f0>), ('pointer_gen', <absl.flags._flag.BooleanFlag object at 0x13b9db780>), ('coverage', <absl.flags._flag.BooleanFlag object at 0x13b9dba20>), ('cov_loss_wt', <absl.flags._flag.Flag object at 0x13b9db908>), ('convert_to_coverage_model', <absl.flags._flag.BooleanFlag object at 0x13b9db978>), ('restore_best_model', <absl.flags._flag.BooleanFlag object at 0x13b9dbac8>), ('debug', <absl.flags._flag.BooleanFlag object at 0x13b9dbc50>)])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS.__flags.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
    "\n",
    "  # The CoQA annotations are character based. We first project them to\n",
    "  # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
    "  # often find a \"better match\". For example:\n",
    "  #\n",
    "  #   Question: What year was John Smith born?\n",
    "  #   Context: The leader was John Smith (1895-1943).\n",
    "  #   Answer: 1895\n",
    "  #\n",
    "  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
    "  # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
    "  # the exact answer, 1895.\n",
    "  #\n",
    "  # However, this is not always possible. Consider the following:\n",
    "  #\n",
    "  #   Question: What country is the top exporter of electornics?\n",
    "  #   Context: The Japanese electronics industry is the lagest in the world.\n",
    "  #   Answer: Japan\n",
    "  #\n",
    "  # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
    "  # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
    "  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
    "  # in CoQA, but does happen.\n",
    "  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "  for new_start in range(input_start, input_end + 1):\n",
    "    for new_end in range(input_end, new_start - 1, -1):\n",
    "      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "      if text_span == tok_answer_text:\n",
    "        return (new_start, new_end)\n",
    "\n",
    "  return (input_start, input_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "  # Because of the sliding window approach taken to scoring documents, a single\n",
    "  # token can appear in multiple documents. E.g.\n",
    "  #  Doc: the man went to the store and bought a gallon of milk\n",
    "  #  Span A: the man went to the\n",
    "  #  Span B: to the store and bought\n",
    "  #  Span C: and bought a gallon of\n",
    "  #  ...\n",
    "  #\n",
    "  # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "  # want to consider the score with \"maximum context\", which we define as\n",
    "  # the *minimum* of its left and right context (the *sum* of left and\n",
    "  # right context will always be the same, of course).\n",
    "  #\n",
    "  # In the example the maximum context for 'bought' would be span C since\n",
    "  # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "  # and 0 right context.\n",
    "  best_score = None\n",
    "  best_span_index = None\n",
    "  for (span_index, doc_span) in enumerate(doc_spans):\n",
    "    end = doc_span.start + doc_span.length - 1\n",
    "    if position < doc_span.start:\n",
    "      continue\n",
    "    if position > end:\n",
    "      continue\n",
    "    num_left_context = position - doc_span.start\n",
    "    num_right_context = end - position\n",
    "    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "    if best_score is None or score > best_score:\n",
    "      best_score = score\n",
    "      best_span_index = span_index\n",
    "\n",
    "  return cur_span_index == best_span_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureWriter(object):\n",
    "  \"\"\"Writes InputFeature to TF example file.\"\"\"\n",
    "\n",
    "  def __init__(self, filename, is_training):\n",
    "    self.filename = filename\n",
    "    self.is_training = is_training\n",
    "    self.num_features = 0\n",
    "    self._writer = tf.python_io.TFRecordWriter(filename)\n",
    "\n",
    "  def process_feature(self, feature):\n",
    "    \"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"\n",
    "    self.num_features += 1\n",
    "\n",
    "    def create_int_feature(values):\n",
    "      feature = tf.train.Feature(\n",
    "          int64_list=tf.train.Int64List(value=list(values)))\n",
    "      return feature\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"unique_ids\"] = create_int_feature([feature.unique_id])\n",
    "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "\n",
    "    if self.is_training:\n",
    "      features[\"start_positions\"] = create_int_feature([feature.start_position])\n",
    "      features[\"end_positions\"] = create_int_feature([feature.end_position])\n",
    "     \n",
    "    \n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    self._writer.write(tf_example.SerializeToString())\n",
    "\n",
    "  def close(self):\n",
    "    self._writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               unique_id,\n",
    "               example_index,\n",
    "               doc_span_index,\n",
    "               tokens,\n",
    "               token_to_orig_map,\n",
    "               token_is_max_context,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               start_position=None,\n",
    "               end_position=None ):\n",
    "    self.unique_id = unique_id\n",
    "    self.example_index = example_index\n",
    "    self.doc_span_index = doc_span_index\n",
    "    self.tokens = tokens\n",
    "    self.token_to_orig_map = token_to_orig_map\n",
    "    self.token_is_max_context = token_is_max_context\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length,max_qa_history, is_training,\n",
    "                                 output_fn):\n",
    "      \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "      unique_id = 1000000000\n",
    "\n",
    "      for (example_index, example) in enumerate(examples):\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "       \n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "        \n",
    "        qa_history_tokens =tokenizer.tokenize(example.qa_history_text)\n",
    "        \n",
    "        if len(qa_history_tokens) > max_qa_history:\n",
    "            query_tokens = query_tokens[0:max_qa_history]\n",
    "            \n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = []\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "         \n",
    "        if is_training:\n",
    "          tok_start_position = orig_to_tok_index[example.start_position]\n",
    "          if example.end_position < len(example.doc_tokens) - 1:\n",
    "            tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "          else:\n",
    "            tok_end_position = len(all_doc_tokens) - 1\n",
    "          (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "              all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "              example.orig_answer_text)\n",
    "\n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - max_qa_history - 3\n",
    "\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "          length = len(all_doc_tokens) - start_offset\n",
    "          if length > max_tokens_for_doc:\n",
    "            length = max_tokens_for_doc\n",
    "          doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "          if start_offset + length == len(all_doc_tokens):\n",
    "            break\n",
    "          start_offset += min(length, doc_stride)\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "          tokens = []\n",
    "          token_to_orig_map = {}\n",
    "          token_is_max_context = {}\n",
    "          segment_ids = []\n",
    "          tokens.append(\"[CLS]\")\n",
    "          segment_ids.append(0)\n",
    "          for token in query_tokens:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "          tokens.append(\"[SEP]\")\n",
    "          segment_ids.append(0)\n",
    "\n",
    "          for i in range(doc_span.length):\n",
    "            split_token_index = doc_span.start + i\n",
    "            token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "            is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                   split_token_index)\n",
    "            token_is_max_context[len(tokens)] = is_max_context\n",
    "            tokens.append(all_doc_tokens[split_token_index])\n",
    "            segment_ids.append(1)\n",
    "            \n",
    "          for token in qa_history_tokens:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "          \n",
    "          tokens.append(\"[SEP]\")\n",
    "          segment_ids.append(1)\n",
    "        \n",
    "\n",
    "          input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "          # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "          # tokens are attended to.\n",
    "          input_mask = [1] * len(input_ids)\n",
    "\n",
    "          # Zero-pad up to the sequence length.\n",
    "          while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "          assert len(input_ids) == max_seq_length\n",
    "          assert len(input_mask) == max_seq_length\n",
    "          assert len(segment_ids) == max_seq_length\n",
    "\n",
    "          start_position = None\n",
    "          end_position = None\n",
    "          if is_training :\n",
    "            # For training, if our document chunk does not contain an annotation\n",
    "            # we throw it out, since there is nothing to predict.\n",
    "            doc_start = doc_span.start\n",
    "            doc_end = doc_span.start + doc_span.length - 1\n",
    "            out_of_span = False\n",
    "            if not (tok_start_position >= doc_start and\n",
    "                    tok_end_position <= doc_end):\n",
    "              out_of_span = True\n",
    "            if out_of_span:\n",
    "              start_position = 0\n",
    "              end_position = 0\n",
    "            else:\n",
    "              doc_offset = len(query_tokens) + 2\n",
    "              start_position = tok_start_position - doc_start + doc_offset\n",
    "              end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "          \n",
    "\n",
    "          if example_index < 20:\n",
    "            print(\"*** Example ***\")\n",
    "            print(\"unique_id: %s\" % (unique_id))\n",
    "            print(\"example_index: %s\" % (example_index))\n",
    "            print(\"doc_span_index: %s\" % (doc_span_index))\n",
    "            print(\"tokens: %s\" % \" \".join(\n",
    "                [tokenization.printable_text(x) for x in tokens]))\n",
    "            print(\"token_to_orig_map: %s\" % \" \".join(\n",
    "                [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
    "            print(\"token_is_max_context: %s\" % \" \".join([\n",
    "                \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
    "            ]))\n",
    "            print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            print(\n",
    "                \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            print(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            \n",
    "            if is_training:\n",
    "              answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "              print(\"start_position: %d\" % (start_position))\n",
    "              print(\"end_position: %d\" % (end_position))\n",
    "              print(\n",
    "                  \"answer: %s\" % (tokenization.printable_text(answer_text)))\n",
    "\n",
    "          feature = InputFeatures(\n",
    "              unique_id=unique_id,\n",
    "              example_index=example_index,\n",
    "              doc_span_index=doc_span_index,\n",
    "              tokens=tokens,\n",
    "              token_to_orig_map=token_to_orig_map,\n",
    "              token_is_max_context=token_is_max_context,\n",
    "              input_ids=input_ids,\n",
    "              input_mask=input_mask,\n",
    "              segment_ids=segment_ids,\n",
    "              start_position=start_position,\n",
    "              end_position=end_position )\n",
    "\n",
    "          # Run callback\n",
    "          output_fn(feature)\n",
    "\n",
    "          unique_id += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 use_one_hot_embeddings):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "  model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids,\n",
    "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "  final_hidden = model.get_sequence_output()\n",
    "\n",
    "  final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=3)\n",
    "  batch_size = final_hidden_shape[0]\n",
    "  seq_length = final_hidden_shape[1]\n",
    "  hidden_size = final_hidden_shape[2]\n",
    "\n",
    "  output_weights = tf.get_variable(\n",
    "      \"cls/coqa/output_weights\", [2, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"cls/coqa/output_bias\", [2], initializer=tf.zeros_initializer())\n",
    "\n",
    "  final_hidden_matrix = tf.reshape(final_hidden,\n",
    "                                   [batch_size * seq_length, hidden_size])\n",
    "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
    "  logits = tf.nn.bias_add(logits, output_bias)\n",
    "\n",
    "  logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
    "  logits = tf.transpose(logits, [2, 0, 1])\n",
    "\n",
    "  unstacked_logits = tf.unstack(logits, axis=0)\n",
    "\n",
    "  (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
    "\n",
    "  return (start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    tf.logging.info(\"*** Features ***\")\n",
    "    for name in sorted(features.keys()):\n",
    "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "    unique_ids = features[\"unique_ids\"]\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    (start_logits, end_logits) = create_model(\n",
    "        bert_config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      seq_length = modeling.get_shape_list(input_ids)[1]\n",
    "\n",
    "      def compute_loss(logits, positions):\n",
    "        one_hot_positions = tf.one_hot(\n",
    "            positions, depth=seq_length, dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -tf.reduce_mean(\n",
    "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
    "        return loss\n",
    "\n",
    "      start_positions = features[\"start_positions\"]\n",
    "      end_positions = features[\"end_positions\"]\n",
    "\n",
    "      start_loss = compute_loss(start_logits, start_positions)\n",
    "      end_loss = compute_loss(end_logits, end_positions)\n",
    "\n",
    "      total_loss = (start_loss + end_loss) / 2.0\n",
    "\n",
    "      train_op = optimization.create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      predictions = {\n",
    "          \"unique_ids\": unique_ids,\n",
    "          \"start_logits\": start_logits,\n",
    "          \"end_logits\": end_logits,\n",
    "      }\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
    "\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  name_to_features = {\n",
    "      \"unique_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  }\n",
    "\n",
    "  if is_training:\n",
    "    name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "\n",
    "  def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "      t = example[name]\n",
    "      if t.dtype == tf.int64:\n",
    "        t = tf.to_int32(t)\n",
    "      example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))\n",
    "\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length, do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file):\n",
    "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "  tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
    "  tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
    "\n",
    "  example_index_to_features = collections.defaultdict(list)\n",
    "  for feature in all_features:\n",
    "    example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "  unique_id_to_result = {}\n",
    "  for result in all_results:\n",
    "    unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "      \"PrelimPrediction\",\n",
    "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "  all_predictions = collections.OrderedDict()\n",
    "  all_nbest_json = collections.OrderedDict()\n",
    "  scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "  for (example_index, example) in enumerate(all_examples):\n",
    "    features = example_index_to_features[example_index]\n",
    "\n",
    "    prelim_predictions = []\n",
    "    # keep track of the minimum score of null start+end of position 0\n",
    "    score_null = 1000000  # large and positive\n",
    "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
    "    null_start_logit = 0  # the start logit at the slice with min null score\n",
    "    null_end_logit = 0  # the end logit at the slice with min null score\n",
    "    for (feature_index, feature) in enumerate(features):\n",
    "      result = unique_id_to_result[feature.unique_id]\n",
    "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "      # if we could have irrelevant answers, get the min score of irrelevant\n",
    "      if FLAGS.version_2_with_negative:\n",
    "        feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "        if feature_null_score < score_null:\n",
    "          score_null = feature_null_score\n",
    "          min_null_feature_index = feature_index\n",
    "          null_start_logit = result.start_logits[0]\n",
    "          null_end_logit = result.end_logits[0]\n",
    "      for start_index in start_indexes:\n",
    "        for end_index in end_indexes:\n",
    "          # We could hypothetically create invalid predictions, e.g., predict\n",
    "          # that the start of the span is in the question. We throw out all\n",
    "          # invalid predictions.\n",
    "          if start_index >= len(feature.tokens):\n",
    "            continue\n",
    "          if end_index >= len(feature.tokens):\n",
    "            continue\n",
    "          if start_index not in feature.token_to_orig_map:\n",
    "            continue\n",
    "          if end_index not in feature.token_to_orig_map:\n",
    "            continue\n",
    "          if not feature.token_is_max_context.get(start_index, False):\n",
    "            continue\n",
    "          if end_index < start_index:\n",
    "            continue\n",
    "          length = end_index - start_index + 1\n",
    "          if length > max_answer_length:\n",
    "            continue\n",
    "          prelim_predictions.append(\n",
    "              _PrelimPrediction(\n",
    "                  feature_index=feature_index,\n",
    "                  start_index=start_index,\n",
    "                  end_index=end_index,\n",
    "                  start_logit=result.start_logits[start_index],\n",
    "                  end_logit=result.end_logits[end_index]))\n",
    "\n",
    "    if FLAGS.version_2_with_negative:\n",
    "      prelim_predictions.append(\n",
    "          _PrelimPrediction(\n",
    "              feature_index=min_null_feature_index,\n",
    "              start_index=0,\n",
    "              end_index=0,\n",
    "              start_logit=null_start_logit,\n",
    "              end_logit=null_end_logit))\n",
    "    prelim_predictions = sorted(\n",
    "        prelim_predictions,\n",
    "        key=lambda x: (x.start_logit + x.end_logit),\n",
    "        reverse=True)\n",
    "\n",
    "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    seen_predictions = {}\n",
    "    nbest = []\n",
    "    for pred in prelim_predictions:\n",
    "      if len(nbest) >= n_best_size:\n",
    "        break\n",
    "      feature = features[pred.feature_index]\n",
    "      if pred.start_index > 0:  # this is a non-null prediction\n",
    "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "        tok_text = \" \".join(tok_tokens)\n",
    "\n",
    "        # De-tokenize WordPieces that have been split off.\n",
    "        tok_text = tok_text.replace(\" ##\", \"\")\n",
    "        tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "        # Clean whitespace\n",
    "        tok_text = tok_text.strip()\n",
    "        tok_text = \" \".join(tok_text.split())\n",
    "        orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "        final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
    "        if final_text in seen_predictions:\n",
    "          continue\n",
    "\n",
    "        seen_predictions[final_text] = True\n",
    "      else:\n",
    "        final_text = \"\"\n",
    "        seen_predictions[final_text] = True\n",
    "\n",
    "      nbest.append(\n",
    "          _NbestPrediction(\n",
    "              text=final_text,\n",
    "              start_logit=pred.start_logit,\n",
    "              end_logit=pred.end_logit))\n",
    "\n",
    "    # if we didn't inlude the empty option in the n-best, inlcude it\n",
    "    if FLAGS.version_2_with_negative:\n",
    "      if \"\" not in seen_predictions:\n",
    "        nbest.append(\n",
    "            _NbestPrediction(\n",
    "                text=\"\", start_logit=null_start_logit,\n",
    "                end_logit=null_end_logit))\n",
    "    # In very rare edge cases we could have no valid predictions. So we\n",
    "    # just create a nonce prediction in this case to avoid failure.\n",
    "    if not nbest:\n",
    "      nbest.append(\n",
    "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "    assert len(nbest) >= 1\n",
    "\n",
    "    total_scores = []\n",
    "    best_non_null_entry = None\n",
    "    for entry in nbest:\n",
    "      total_scores.append(entry.start_logit + entry.end_logit)\n",
    "      if not best_non_null_entry:\n",
    "        if entry.text:\n",
    "          best_non_null_entry = entry\n",
    "\n",
    "    probs = _compute_softmax(total_scores)\n",
    "\n",
    "    nbest_json = []\n",
    "    for (i, entry) in enumerate(nbest):\n",
    "      output = collections.OrderedDict()\n",
    "      output[\"text\"] = entry.text\n",
    "      output[\"probability\"] = probs[i]\n",
    "      output[\"start_logit\"] = entry.start_logit\n",
    "      output[\"end_logit\"] = entry.end_logit\n",
    "      nbest_json.append(output)\n",
    "\n",
    "    assert len(nbest_json) >= 1\n",
    "\n",
    "    if not FLAGS.version_2_with_negative:\n",
    "      all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "    else:\n",
    "      # predict \"\" iff the null score - the score of best non-null > threshold\n",
    "      score_diff = score_null - best_non_null_entry.start_logit - (\n",
    "          best_non_null_entry.end_logit)\n",
    "      scores_diff_json[example.qas_id] = score_diff\n",
    "      if score_diff > FLAGS.null_score_diff_threshold:\n",
    "        all_predictions[example.qas_id] = \"\"\n",
    "      else:\n",
    "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "\n",
    "    all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
    "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
    "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "  validate_flags_or_throw(bert_config)\n",
    "\n",
    "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "\n",
    "  tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "  tpu_cluster_resolver = None\n",
    "  if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      master=FLAGS.master,\n",
    "      model_dir=FLAGS.output_dir,\n",
    "      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "          num_shards=FLAGS.num_tpu_cores,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  train_examples = None\n",
    "  num_train_steps = None\n",
    "  num_warmup_steps = None\n",
    "  if FLAGS.do_train:\n",
    "    train_examples = read_coqa_examples(\n",
    "        input_file=FLAGS.train_file, is_training=True)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "    # Pre-shuffle the input to avoid having to make a very large shuffle\n",
    "    # buffer in in the `input_fn`.\n",
    "    rng = random.Random(12345)\n",
    "    rng.shuffle(train_examples)\n",
    "\n",
    "  model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=FLAGS.init_checkpoint,\n",
    "      learning_rate=FLAGS.learning_rate,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size=FLAGS.train_batch_size,\n",
    "      predict_batch_size=FLAGS.predict_batch_size)\n",
    "\n",
    "  if FLAGS.do_train:\n",
    "    # We write to a temporary file to avoid storing very large constant tensors\n",
    "    # in memory.\n",
    "    train_writer = FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"train.tf_record\"),\n",
    "        is_training=True)\n",
    "    convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        doc_stride=FLAGS.doc_stride,\n",
    "        max_query_length=FLAGS.max_query_length,\n",
    "        is_training=True,\n",
    "        output_fn=train_writer.process_feature)\n",
    "    train_writer.close()\n",
    "\n",
    "    tf.logging.info(\"***** Running training *****\")\n",
    "    tf.logging.info(\"  Num orig examples = %d\", len(train_examples))\n",
    "    tf.logging.info(\"  Num split examples = %d\", train_writer.num_features)\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    del train_examples\n",
    "\n",
    "    train_input_fn = input_fn_builder(\n",
    "        input_file=train_writer.filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "  if FLAGS.do_predict:\n",
    "    eval_examples = read_coqa_examples(\n",
    "        input_file=FLAGS.predict_file, is_training=False)\n",
    "\n",
    "    eval_writer = FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "        is_training=False)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "      eval_features.append(feature)\n",
    "      eval_writer.process_feature(feature)\n",
    "\n",
    "    convert_examples_to_features(\n",
    "        examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        doc_stride=FLAGS.doc_stride,\n",
    "        max_query_length=FLAGS.max_query_length,\n",
    "        is_training=False,\n",
    "        output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "\n",
    "    tf.logging.info(\"***** Running predictions *****\")\n",
    "    tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "    tf.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    predict_input_fn = input_fn_builder(\n",
    "        input_file=eval_writer.filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    # If running eval on the TPU, you will need to specify the number of\n",
    "    # steps.\n",
    "    all_results = []\n",
    "    for result in estimator.predict(\n",
    "        predict_input_fn, yield_single_examples=True):\n",
    "      if len(all_results) % 1000 == 0:\n",
    "        tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
    "      unique_id = int(result[\"unique_ids\"])\n",
    "      start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
    "      end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
    "      all_results.append(\n",
    "          RawResult(\n",
    "              unique_id=unique_id,\n",
    "              start_logits=start_logits,\n",
    "              end_logits=end_logits))\n",
    "\n",
    "    output_prediction_file = os.path.join(FLAGS.output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join(FLAGS.output_dir, \"nbest_predictions.json\")\n",
    "    output_null_log_odds_file = os.path.join(FLAGS.output_dir, \"null_odds.json\")\n",
    "\n",
    "    write_predictions(eval_examples, eval_features, all_results,\n",
    "                      FLAGS.n_best_size, FLAGS.max_answer_length,\n",
    "                      FLAGS.do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e15a5a9d387b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#flags.mark_flag_as_required(\"output_dir\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#tf.app.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b477b8d04280>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mbert_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mvalidate_flags_or_throw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 633\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "#flags.mark_flag_as_required(\"vocab_file\")\n",
    "#flags.mark_flag_as_required(\"bert_config_file\")\n",
    "#flags.mark_flag_as_required(\"output_dir\")\n",
    "#tf.app.run()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2]\n",
      "[[[1 2]\n",
      "  [4 5]\n",
      "  [4 5]]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 4,4]])\n",
    "y = tf.constant([[2, 5,5]])  \n",
    "w=tf.stack([x, y], axis=2)  # [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "indices = tf.constant([[1,4], [2,3], [2,1], [1,7]])\n",
    "updates = tf.constant([9, 10, 11, 12])\n",
    "shape = tf.constant([4,8])\n",
    "scatter = tf.scatter_nd(indices, updates, shape)\n",
    " \n",
    "with tf.Session() as sess:\n",
    "  print( sess.run(tf.shape(w)))\n",
    "  print( sess.run( (w)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
