{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dnc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4f72099c4486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdnc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Ensure values are greater than epsilon to avoid numerical instability.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dnc'"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 Google Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"DNC addressing modules.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "from dnc import util\n",
    "\n",
    "# Ensure values are greater than epsilon to avoid numerical instability.\n",
    "_EPSILON = 1e-6\n",
    "\n",
    "TemporalLinkageState = collections.namedtuple('TemporalLinkageState',\n",
    "                                              ('link', 'precedence_weights'))\n",
    "\n",
    "\n",
    "def _vector_norms(m):\n",
    "  squared_norms = tf.reduce_sum(m * m, axis=2, keepdims=True)\n",
    "  return tf.sqrt(squared_norms + _EPSILON)\n",
    "\n",
    "\n",
    "def weighted_softmax(activations, strengths, strengths_op):\n",
    "  \"\"\"Returns softmax over activations multiplied by positive strengths.\n",
    "\n",
    "  Args:\n",
    "    activations: A tensor of shape `[batch_size, num_heads, memory_size]`, of\n",
    "      activations to be transformed. Softmax is taken over the last dimension.\n",
    "    strengths: A tensor of shape `[batch_size, num_heads]` containing strengths to\n",
    "      multiply by the activations prior to the softmax.\n",
    "    strengths_op: An operation to transform strengths before softmax.\n",
    "\n",
    "  Returns:\n",
    "    A tensor of same shape as `activations` with weighted softmax applied.\n",
    "  \"\"\"\n",
    "  transformed_strengths = tf.expand_dims(strengths_op(strengths), -1)\n",
    "  sharp_activations = activations * transformed_strengths\n",
    "  softmax = snt.BatchApply(module_or_op=tf.nn.softmax)\n",
    "  return softmax(sharp_activations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"DNC addressing modules.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "from dnc import util\n",
    "\n",
    "# Ensure values are greater than epsilon to avoid numerical instability.\n",
    "_EPSILON = 1e-6\n",
    "\n",
    "TemporalLinkageState = collections.namedtuple('TemporalLinkageState',\n",
    "                                              ('link', 'precedence_weights'))\n",
    "\n",
    "\n",
    "def _vector_norms(m):\n",
    "  squared_norms = tf.reduce_sum(m * m, axis=2, keepdims=True)\n",
    "  return tf.sqrt(squared_norms + _EPSILON)\n",
    "\n",
    "\n",
    "def weighted_softmax(activations, strengths, strengths_op):\n",
    "  \"\"\"Returns softmax over activations multiplied by positive strengths.\n",
    "\n",
    "  Args:\n",
    "    activations: A tensor of shape `[batch_size, num_heads, memory_size]`, of\n",
    "      activations to be transformed. Softmax is taken over the last dimension.\n",
    "    strengths: A tensor of shape `[batch_size, num_heads]` containing strengths to\n",
    "      multiply by the activations prior to the softmax.\n",
    "    strengths_op: An operation to transform strengths before softmax.\n",
    "\n",
    "  Returns:\n",
    "    A tensor of same shape as `activations` with weighted softmax applied.\n",
    "  \"\"\"\n",
    "  transformed_strengths = tf.expand_dims(strengths_op(strengths), -1)\n",
    "  sharp_activations = activations * transformed_strengths\n",
    "  softmax = snt.BatchApply(module_or_op=tf.nn.softmax)\n",
    "  return softmax(sharp_activations)\n",
    "\n",
    "\n",
    "class CosineWeights(snt.AbstractModule):\n",
    "  \"\"\"Cosine-weighted attention.\n",
    "\n",
    "  Calculates the cosine similarity between a query and each word in memory, then\n",
    "  applies a weighted softmax to return a sharp distribution.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads,\n",
    "               word_size,\n",
    "               strength_op=tf.nn.softplus,\n",
    "               name='cosine_weights'):\n",
    "    \"\"\"Initializes the CosineWeights module.\n",
    "\n",
    "    Args:\n",
    "      num_heads: number of memory heads.\n",
    "      word_size: memory word size.\n",
    "      strength_op: operation to apply to strengths (default is tf.nn.softplus).\n",
    "      name: module name (default 'cosine_weights')\n",
    "    \"\"\"\n",
    "    super(CosineWeights, self).__init__(name=name)\n",
    "    self._num_heads = num_heads\n",
    "    self._word_size = word_size\n",
    "    self._strength_op = strength_op\n",
    "\n",
    "  def _build(self, memory, keys, strengths):\n",
    "    \"\"\"Connects the CosineWeights module into the graph.\n",
    "\n",
    "    Args:\n",
    "      memory: A 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n",
    "      keys: A 3-D tensor of shape `[batch_size, num_heads, word_size]`.\n",
    "      strengths: A 2-D tensor of shape `[batch_size, num_heads]`.\n",
    "\n",
    "    Returns:\n",
    "      Weights tensor of shape `[batch_size, num_heads, memory_size]`.\n",
    "    \"\"\"\n",
    "    # Calculates the inner product between the query vector and words in memory.\n",
    "    dot = tf.matmul(keys, memory, adjoint_b=True)\n",
    "\n",
    "    # Outer product to compute denominator (euclidean norm of query and memory).\n",
    "    memory_norms = _vector_norms(memory)\n",
    "    key_norms = _vector_norms(keys)\n",
    "    norm = tf.matmul(key_norms, memory_norms, adjoint_b=True)\n",
    "\n",
    "    # Calculates cosine similarity between the query vector and words in memory.\n",
    "    similarity = dot / (norm + _EPSILON)\n",
    "\n",
    "    return weighted_softmax(similarity, strengths, self._strength_op)\n",
    "\n",
    "\n",
    "class TemporalLinkage(snt.RNNCore):\n",
    "  \"\"\"Keeps track of write order for forward and backward addressing.\n",
    "\n",
    "  This is a pseudo-RNNCore module, whose state is a pair `(link,\n",
    "  precedence_weights)`, where `link` is a (collection of) graphs for (possibly\n",
    "  multiple) write heads (represented by a tensor with values in the range\n",
    "  [0, 1]), and `precedence_weights` records the \"previous write locations\" used\n",
    "  to build the link graphs.\n",
    "\n",
    "  The function `directional_read_weights` computes addresses following the\n",
    "  forward and backward directions in the link graphs.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, memory_size, num_writes, name='temporal_linkage'):\n",
    "    \"\"\"Construct a TemporalLinkage module.\n",
    "\n",
    "    Args:\n",
    "      memory_size: The number of memory slots.\n",
    "      num_writes: The number of write heads.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super(TemporalLinkage, self).__init__(name=name)\n",
    "    self._memory_size = memory_size\n",
    "    self._num_writes = num_writes\n",
    "\n",
    "  def _build(self, write_weights, prev_state):\n",
    "    \"\"\"Calculate the updated linkage state given the write weights.\n",
    "\n",
    "    Args:\n",
    "      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          containing the memory addresses of the different write heads.\n",
    "      prev_state: `TemporalLinkageState` tuple containg a tensor `link` of\n",
    "          shape `[batch_size, num_writes, memory_size, memory_size]`, and a\n",
    "          tensor `precedence_weights` of shape `[batch_size, num_writes,\n",
    "          memory_size]` containing the aggregated history of recent writes.\n",
    "\n",
    "    Returns:\n",
    "      A `TemporalLinkageState` tuple `next_state`, which contains the updated\n",
    "      link and precedence weights.\n",
    "    \"\"\"\n",
    "    link = self._link(prev_state.link, prev_state.precedence_weights,\n",
    "                      write_weights)\n",
    "    precedence_weights = self._precedence_weights(prev_state.precedence_weights,\n",
    "                                                  write_weights)\n",
    "    return TemporalLinkageState(\n",
    "        link=link, precedence_weights=precedence_weights)\n",
    "\n",
    "  def directional_read_weights(self, link, prev_read_weights, forward):\n",
    "    \"\"\"Calculates the forward or the backward read weights.\n",
    "\n",
    "    For each read head (at a given address), there are `num_writes` link graphs\n",
    "    to follow. Thus this function computes a read address for each of the\n",
    "    `num_reads * num_writes` pairs of read and write heads.\n",
    "\n",
    "    Args:\n",
    "      link: tensor of shape `[batch_size, num_writes, memory_size,\n",
    "          memory_size]` representing the link graphs L_t.\n",
    "      prev_read_weights: tensor of shape `[batch_size, num_reads,\n",
    "          memory_size]` containing the previous read weights w_{t-1}^r.\n",
    "      forward: Boolean indicating whether to follow the \"future\" direction in\n",
    "          the link graph (True) or the \"past\" direction (False).\n",
    "\n",
    "    Returns:\n",
    "      tensor of shape `[batch_size, num_reads, num_writes, memory_size]`\n",
    "    \"\"\"\n",
    "    with tf.name_scope('directional_read_weights'):\n",
    "      # We calculate the forward and backward directions for each pair of\n",
    "      # read and write heads; hence we need to tile the read weights and do a\n",
    "      # sort of \"outer product\" to get this.\n",
    "      expanded_read_weights = tf.stack([prev_read_weights] * self._num_writes,\n",
    "                                       1)\n",
    "      result = tf.matmul(expanded_read_weights, link, adjoint_b=forward)\n",
    "      # Swap dimensions 1, 2 so order is [batch, reads, writes, memory]:\n",
    "      return tf.transpose(result, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def _link(self, prev_link, prev_precedence_weights, write_weights):\n",
    "    \"\"\"Calculates the new link graphs.\n",
    "\n",
    "    For each write head, the link is a directed graph (represented by a matrix\n",
    "    with entries in range [0, 1]) whose vertices are the memory locations, and\n",
    "    an edge indicates temporal ordering of writes.\n",
    "\n",
    "    Args:\n",
    "      prev_link: A tensor of shape `[batch_size, num_writes, memory_size,\n",
    "          memory_size]` representing the previous link graphs for each write\n",
    "          head.\n",
    "      prev_precedence_weights: A tensor of shape `[batch_size, num_writes,\n",
    "          memory_size]` which is the previous \"aggregated\" write weights for\n",
    "          each write head.\n",
    "      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          containing the new locations in memory written to.\n",
    "\n",
    "    Returns:\n",
    "      A tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n",
    "      containing the new link graphs for each write head.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('link'):\n",
    "      batch_size = tf.shape(prev_link)[0]\n",
    "      write_weights_i = tf.expand_dims(write_weights, 3)\n",
    "      write_weights_j = tf.expand_dims(write_weights, 2)\n",
    "      prev_precedence_weights_j = tf.expand_dims(prev_precedence_weights, 2)\n",
    "      prev_link_scale = 1 - write_weights_i - write_weights_j\n",
    "      new_link = write_weights_i * prev_precedence_weights_j\n",
    "      link = prev_link_scale * prev_link + new_link\n",
    "      # Return the link with the diagonal set to zero, to remove self-looping\n",
    "      # edges.\n",
    "      return tf.matrix_set_diag(\n",
    "          link,\n",
    "          tf.zeros(\n",
    "              [batch_size, self._num_writes, self._memory_size],\n",
    "              dtype=link.dtype))\n",
    "\n",
    "  def _precedence_weights(self, prev_precedence_weights, write_weights):\n",
    "    \"\"\"Calculates the new precedence weights given the current write weights.\n",
    "\n",
    "    The precedence weights are the \"aggregated write weights\" for each write\n",
    "    head, where write weights with sum close to zero will leave the precedence\n",
    "    weights unchanged, but with sum close to one will replace the precedence\n",
    "    weights.\n",
    "\n",
    "    Args:\n",
    "      prev_precedence_weights: A tensor of shape `[batch_size, num_writes,\n",
    "          memory_size]` containing the previous precedence weights.\n",
    "      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          containing the new write weights.\n",
    "\n",
    "    Returns:\n",
    "      A tensor of shape `[batch_size, num_writes, memory_size]` containing the\n",
    "      new precedence weights.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('precedence_weights'):\n",
    "      write_sum = tf.reduce_sum(write_weights, 2, keepdims=True)\n",
    "      return (1 - write_sum) * prev_precedence_weights + write_weights\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    \"\"\"Returns a `TemporalLinkageState` tuple of the state tensors' shapes.\"\"\"\n",
    "    return TemporalLinkageState(\n",
    "        link=tf.TensorShape(\n",
    "            [self._num_writes, self._memory_size, self._memory_size]),\n",
    "        precedence_weights=tf.TensorShape([self._num_writes,\n",
    "                                           self._memory_size]),)\n",
    "\n",
    "\n",
    "class Freeness(snt.RNNCore):\n",
    "  \"\"\"Memory usage that is increased by writing and decreased by reading.\n",
    "\n",
    "  This module is a pseudo-RNNCore whose state is a tensor with values in\n",
    "  the range [0, 1] indicating the usage of each of `memory_size` memory slots.\n",
    "\n",
    "  The usage is:\n",
    "\n",
    "  *   Increased by writing, where usage is increased towards 1 at the write\n",
    "      addresses.\n",
    "  *   Decreased by reading, where usage is decreased after reading from a\n",
    "      location when free_gate is close to 1.\n",
    "\n",
    "  The function `write_allocation_weights` can be invoked to get free locations\n",
    "  to write to for a number of write heads.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, memory_size, name='freeness'):\n",
    "    \"\"\"Creates a Freeness module.\n",
    "\n",
    "    Args:\n",
    "      memory_size: Number of memory slots.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super(Freeness, self).__init__(name=name)\n",
    "    self._memory_size = memory_size\n",
    "\n",
    "  def _build(self, write_weights, free_gate, read_weights, prev_usage):\n",
    "    \"\"\"Calculates the new memory usage u_t.\n",
    "\n",
    "    Memory that was written to in the previous time step will have its usage\n",
    "    increased; memory that was read from and the controller says can be \"freed\"\n",
    "    will have its usage decreased.\n",
    "\n",
    "    Args:\n",
    "      write_weights: tensor of shape `[batch_size, num_writes,\n",
    "          memory_size]` giving write weights at previous time step.\n",
    "      free_gate: tensor of shape `[batch_size, num_reads]` which indicates\n",
    "          which read heads read memory that can now be freed.\n",
    "      read_weights: tensor of shape `[batch_size, num_reads,\n",
    "          memory_size]` giving read weights at previous time step.\n",
    "      prev_usage: tensor of shape `[batch_size, memory_size]` giving\n",
    "          usage u_{t - 1} at the previous time step, with entries in range\n",
    "          [0, 1].\n",
    "\n",
    "    Returns:\n",
    "      tensor of shape `[batch_size, memory_size]` representing updated memory\n",
    "      usage.\n",
    "    \"\"\"\n",
    "    # Calculation of usage is not differentiable with respect to write weights.\n",
    "    write_weights = tf.stop_gradient(write_weights)\n",
    "    usage = self._usage_after_write(prev_usage, write_weights)\n",
    "    usage = self._usage_after_read(usage, free_gate, read_weights)\n",
    "    return usage\n",
    "\n",
    "  def write_allocation_weights(self, usage, write_gates, num_writes):\n",
    "    \"\"\"Calculates freeness-based locations for writing to.\n",
    "\n",
    "    This finds unused memory by ranking the memory locations by usage, for each\n",
    "    write head. (For more than one write head, we use a \"simulated new usage\"\n",
    "    which takes into account the fact that the previous write head will increase\n",
    "    the usage in that area of the memory.)\n",
    "\n",
    "    Args:\n",
    "      usage: A tensor of shape `[batch_size, memory_size]` representing\n",
    "          current memory usage.\n",
    "      write_gates: A tensor of shape `[batch_size, num_writes]` with values in\n",
    "          the range [0, 1] indicating how much each write head does writing\n",
    "          based on the address returned here (and hence how much usage\n",
    "          increases).\n",
    "      num_writes: The number of write heads to calculate write weights for.\n",
    "\n",
    "    Returns:\n",
    "      tensor of shape `[batch_size, num_writes, memory_size]` containing the\n",
    "          freeness-based write locations. Note that this isn't scaled by\n",
    "          `write_gate`; this scaling must be applied externally.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('write_allocation_weights'):\n",
    "      # expand gatings over memory locations\n",
    "      write_gates = tf.expand_dims(write_gates, -1)\n",
    "\n",
    "      allocation_weights = []\n",
    "      for i in range(num_writes):\n",
    "        allocation_weights.append(self._allocation(usage))\n",
    "        # update usage to take into account writing to this new allocation\n",
    "        usage += ((1 - usage) * write_gates[:, i, :] * allocation_weights[i])\n",
    "\n",
    "      # Pack the allocation weights for the write heads into one tensor.\n",
    "      return tf.stack(allocation_weights, axis=1)\n",
    "\n",
    "  def _usage_after_write(self, prev_usage, write_weights):\n",
    "    \"\"\"Calcualtes the new usage after writing to memory.\n",
    "\n",
    "    Args:\n",
    "      prev_usage: tensor of shape `[batch_size, memory_size]`.\n",
    "      write_weights: tensor of shape `[batch_size, num_writes, memory_size]`.\n",
    "\n",
    "    Returns:\n",
    "      New usage, a tensor of shape `[batch_size, memory_size]`.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('usage_after_write'):\n",
    "      # Calculate the aggregated effect of all write heads\n",
    "      write_weights = 1 - util.reduce_prod(1 - write_weights, 1)\n",
    "      return prev_usage + (1 - prev_usage) * write_weights\n",
    "\n",
    "  def _usage_after_read(self, prev_usage, free_gate, read_weights):\n",
    "    \"\"\"Calcualtes the new usage after reading and freeing from memory.\n",
    "\n",
    "    Args:\n",
    "      prev_usage: tensor of shape `[batch_size, memory_size]`.\n",
    "      free_gate: tensor of shape `[batch_size, num_reads]` with entries in the\n",
    "          range [0, 1] indicating the amount that locations read from can be\n",
    "          freed.\n",
    "      read_weights: tensor of shape `[batch_size, num_reads, memory_size]`.\n",
    "\n",
    "    Returns:\n",
    "      New usage, a tensor of shape `[batch_size, memory_size]`.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('usage_after_read'):\n",
    "      free_gate = tf.expand_dims(free_gate, -1)\n",
    "      free_read_weights = free_gate * read_weights\n",
    "      phi = util.reduce_prod(1 - free_read_weights, 1, name='phi')\n",
    "      return prev_usage * phi\n",
    "\n",
    "  def _allocation(self, usage):\n",
    "    r\"\"\"Computes allocation by sorting `usage`.\n",
    "\n",
    "    This corresponds to the value a = a_t[\\phi_t[j]] in the paper.\n",
    "\n",
    "    Args:\n",
    "      usage: tensor of shape `[batch_size, memory_size]` indicating current\n",
    "          memory usage. This is equal to u_t in the paper when we only have one\n",
    "          write head, but for multiple write heads, one should update the usage\n",
    "          while iterating through the write heads to take into account the\n",
    "          allocation returned by this function.\n",
    "\n",
    "    Returns:\n",
    "      Tensor of shape `[batch_size, memory_size]` corresponding to allocation.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('allocation'):\n",
    "      # Ensure values are not too small prior to cumprod.\n",
    "      usage = _EPSILON + (1 - _EPSILON) * usage\n",
    "\n",
    "      nonusage = 1 - usage\n",
    "      sorted_nonusage, indices = tf.nn.top_k(\n",
    "          nonusage, k=self._memory_size, name='sort')\n",
    "      sorted_usage = 1 - sorted_nonusage\n",
    "      prod_sorted_usage = tf.cumprod(sorted_usage, axis=1, exclusive=True)\n",
    "      sorted_allocation = sorted_nonusage * prod_sorted_usage\n",
    "      inverse_indices = util.batch_invert_permutation(indices)\n",
    "\n",
    "      # This final line \"unsorts\" sorted_allocation, so that the indexing\n",
    "      # corresponds to the original indexing of `usage`.\n",
    "      return util.batch_gather(sorted_allocation, inverse_indices)\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    \"\"\"Returns the shape of the state tensor.\"\"\"\n",
    "    return tf.TensorShape([self._memory_size])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
