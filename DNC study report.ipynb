{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINT:  Tensor(\"add:0\", shape=(2, 2), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-2-5b021ba28797>:7 f  *\n        b = tf.Variable(12.)\n    /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:260 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:254 _variable_v2_call\n        shape=shape)\n    /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:65 getter\n        return captured_getter(captured_previous, **kwargs)\n    /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:498 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5b021ba28797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2292\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2294\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2626\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2628\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2629\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2515\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2518\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2519\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    941\u001b[0m                                           converted_func)\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-2-5b021ba28797>:7 f  *\n        b = tf.Variable(12.)\n    /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:260 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:254 _variable_v2_call\n        shape=shape)\n    /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:65 getter\n        return captured_getter(captured_previous, **kwargs)\n    /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:498 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def f():\n",
    "    a = tf.constant([[10,10],[11.,1.]])\n",
    "    x = tf.constant([[1.,0.],[0.,1.]])\n",
    "    b = tf.Variable(12.)\n",
    "    y = tf.matmul(a, x) + b\n",
    "    print(\"PRINT: \", y)\n",
    "    tf.print(\"TF-PRINT: \", y)\n",
    "    return y\n",
    "\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINT:  Tensor(\"add:0\", shape=(2, 2), dtype=float32)\n",
      "PRINT:  Tensor(\"add:0\", shape=(2, 2), dtype=float32)\n",
      "TF-PRINT:  [[22 22]\n",
      " [23 13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[22., 22.],\n",
       "       [23., 13.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = None\n",
    "\n",
    "@tf.function\n",
    "def f():\n",
    "    a = tf.constant([[10, 10], [11., 1.]])\n",
    "    x = tf.constant([[1., 0.], [0., 1.]])\n",
    "    global b\n",
    "    if b is None:\n",
    "        b = tf.Variable(12.)\n",
    "    y = tf.matmul(a, x) + b\n",
    "    print(\"PRINT: \", y)\n",
    "    tf.print(\"TF-PRINT: \", y)\n",
    "    return y\n",
    "\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINT:  Tensor(\"add:0\", shape=(2, 2), dtype=float32)\n",
      "PRINT:  Tensor(\"add:0\", shape=(2, 2), dtype=float32)\n",
      "TF-PRINT:  [[22 22]\n",
      " [23 13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[22., 22.],\n",
       "       [23., 13.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class F():\n",
    "    def __init__(self):\n",
    "        self._b = None\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self):\n",
    "        a = tf.constant([[10, 10], [11., 1.]])\n",
    "        x = tf.constant([[1., 0.], [0., 1.]])\n",
    "        if self._b is None:\n",
    "            self._b = tf.Variable(12.)\n",
    "        y = tf.matmul(a, x) + self._b\n",
    "        print(\"PRINT: \", y)\n",
    "        tf.print(\"TF-PRINT: \", y)\n",
    "        return y\n",
    "\n",
    "f = F()\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-24-26eaebb70cfe>(6)f()\n",
      "-> x = x + 1\n",
      "(Pdb) n\n",
      "> <ipython-input-24-26eaebb70cfe>(7)f()\n",
      "-> return x\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> <ipython-input-24-26eaebb70cfe>(7)f()-><tf.Tensor: s...nt32, numpy=2>\n",
      "-> return x\n",
      "(Pdb) n\n",
      "--Return--\n",
      "> /Users/wweschen/tf2/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py(551)__call__()-><tf.Tensor: s...nt32, numpy=2>\n",
      "-> return self._python_function(*args, **kwds)\n",
      "(Pdb) x\n",
      "*** NameError: name 'x' is not defined\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "  if x > 0:\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    x = x + 1\n",
    "  return x\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "f(tf.constant(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0 1 2 3 4]\n",
      " [0 1 2 3 4]\n",
      " [0 1 2 3 4]], shape=(3, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0 0 0 0 0]\n",
      " [1 1 1 1 1]\n",
      " [2 2 2 2 2]], shape=(3, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[0 0 0]\n",
      "  [0 1 1]\n",
      "  [0 2 2]\n",
      "  [0 3 3]\n",
      "  [0 4 3]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [1 1 1]\n",
      "  [1 2 2]\n",
      "  [1 3 3]\n",
      "  [1 4 3]]\n",
      "\n",
      " [[2 0 0]\n",
      "  [2 1 1]\n",
      "  [2 2 2]\n",
      "  [2 3 3]\n",
      "  [2 4 3]]], shape=(3, 5, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 2 3 4 4]\n",
      " [1 2 3 4 4]\n",
      " [1 2 3 4 4]], shape=(3, 5), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=int32, numpy=\n",
       "array([[1, 2, 3, 4, 4],\n",
       "       [1, 2, 3, 4, 4],\n",
       "       [1, 2, 3, 4, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_fetch_element_per_group(data,idx):\n",
    "    #here data is of shape [?,m,n] ? is the data batch size, m - groups, n candidates in each group\n",
    "    #idx is of share [?,m]  select a particular from each group.\n",
    "    \n",
    "    nRows = tf.shape(data)[0]  \n",
    "    print(nRows)\n",
    "    \n",
    "    nCols = tf.constant(tf.shape(data)[1] , dtype=tf.int32) \n",
    "    print(nCols)\n",
    "    \n",
    "    m1 = tf.reshape(tf.tile(tf.range(nCols), [nRows]),\n",
    "                                           shape=[nRows, nCols])\n",
    "    print(m1)\n",
    "    m2 = tf.transpose(tf.reshape(tf.tile(tf.range(nRows), [nCols]),\n",
    "                                            shape=[nCols, nRows]))\n",
    "    print(m2)\n",
    "    indices = tf.stack([m2, m1, idx], axis=-1)\n",
    "    # indices should be of shape [?, 5, 3] with indices[i,j]==[i,j,idx[i,j]]\n",
    "    print(indices)\n",
    "    output = tf.gather_nd(data, indices=indices)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "data = tf.constant([[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]],\n",
    "                   [[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]],\n",
    "                   [[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]])\n",
    "\n",
    "idx= tf.constant ([[0,1,2,3,3],[0,1,2,3,3],[0,1,2,3,3]])\n",
    "\n",
    "data1 = tf.constant([[1,2,3,4],\n",
    "                   [1,2,3,4],\n",
    "                   [1,2,3,4]])\n",
    "\n",
    "idx1= tf.constant ([[0],[0],[0]])\n",
    "\n",
    "\n",
    "batch_fetch_element_per_group(data,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Square_5:0\", shape=(None, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N_CHANNELS = 5\n",
    "pl=tf.placeholder(dtype=tf.int32, shape=(None, 28, 28, N_CHANNELS))\n",
    "\n",
    "# Indices we'll use. batch_size = 4 here.\n",
    "label_predictions = tf.constant([0, 2, 0, 3])\n",
    "\n",
    "# Indices of shape [?, 2], with indices[i] = [i, self.label_predictions[i]],\n",
    "# which is easy to do with tf.range() and tf.stack()\n",
    "indices = tf.stack([tf.range(tf.size(label_predictions)), label_predictions], axis=-1)\n",
    "# [[0, 0], [1, 2], [2, 0], [3, 3]]\n",
    "\n",
    "transposed = tf.transpose(pl, perm=[0, 3, 1, 2])\n",
    "gathered = tf.gather_nd(transposed, indices)  # Should be of shape (4, 2, 3)\n",
    "result = tf.expand_dims(gathered, -1)\n",
    "\n",
    "initial_value = np.arange(4*28*28*N_CHANNELS).reshape((4, 28, 28, N_CHANNELS))\n",
    "sess = tf.InteractiveSession()\n",
    "res = sess.run(result, feed_dict={pl: initial_value})\n",
    "# print(res)\n",
    "\n",
    "print(\"checking validity\")\n",
    "for i in range(4):\n",
    "    for x in range(28):\n",
    "        print(x)\n",
    "        for y in range(28):\n",
    "            assert res[i, x, y, 0] == initial_value[i, x, y, indices[i, 1].eval()]\n",
    "print(\"All assertions passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3 4 5 6]\n",
      " [9 8 7 0 0]], shape=(2, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 1 1 1 1]\n",
      " [1 1 1 0 0]], shape=(2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ids=tf.constant([[2,3,4,5,6],[9,8,7,6,5]])\n",
    " \n",
    "start_logits= tf.constant([[0.5,0.1,0.1,0.1,0.1],[0.6,0.1,0.1,0.1,0.1]])\n",
    "end_logits= tf.constant([[0.1,0.1,0.1,0.1,0.7],[0.2,0.1,0.7,0.1,0.1]])\n",
    "\n",
    "_, starts= tf.nn.top_k(start_logits, k=1)\n",
    "_, ends = tf.nn.top_k(end_logits, k=1)\n",
    "\n",
    "spanarray = []\n",
    "maskarray =[]\n",
    "\n",
    "starts = tf.unstack(starts, axis=0)\n",
    "ends = tf.unstack(ends, axis=0)\n",
    "ids = tf.unstack(ids, axis=0)\n",
    "batch_size = len(ids)\n",
    "str_len = len(ids[0])\n",
    "for i in range(batch_size):\n",
    "    spanarray.append(tf.strided_slice(ids[i], starts[i], ends[i] + 1))\n",
    "    maskarray.append(tf.strided_slice(tf.fill([str_len],1),starts[i], ends[i] + 1))\n",
    "    for j in range(str_len - len(spanarray[i])):\n",
    "        spanarray[i] = tf.concat([spanarray[i], [0]], axis=0)\n",
    "        maskarray[i] = tf.concat([maskarray[i], [0]], axis=0)\n",
    "\n",
    "spans = tf.stack(spanarray, axis=0)\n",
    "masks= tf.stack(maskarray, axis=0)\n",
    "\n",
    "print(spans)\n",
    "print(masks)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
       " array([[2, 3, 4, 5, 6],\n",
       "        [9, 8, 7, 0, 0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 0]], dtype=int32)>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_best_span_prediction(ids,start_logits, end_logits):\n",
    "\n",
    "\n",
    "    _, starts= tf.nn.top_k(start_logits, k=1)\n",
    "    _, ends = tf.nn.top_k(end_logits, k=1)\n",
    "\n",
    "    spanarray = []\n",
    "    maskarray =[]\n",
    "\n",
    "    starts = tf.unstack(starts, axis=0)\n",
    "    ends = tf.unstack(ends, axis=0)\n",
    "    ids = tf.unstack(ids, axis=0)\n",
    "    batch_size = len(ids)\n",
    "    str_len = len(ids[0])\n",
    "    for i in range(batch_size):\n",
    "        spanarray.append(tf.strided_slice(ids[i], starts[i], ends[i] + 1))\n",
    "        maskarray.append(tf.strided_slice(tf.fill([str_len],1),starts[i], ends[i] + 1))\n",
    "        for j in range(str_len - len(spanarray[i])):\n",
    "            spanarray[i] = tf.concat([spanarray[i], [0]], axis=0)\n",
    "            maskarray[i] = tf.concat([maskarray[i], [0]], axis=0)\n",
    "\n",
    "    spans = tf.stack(spanarray, axis=0)\n",
    "    masks= tf.stack(maskarray, axis=0)\n",
    "    return spans,masks\n",
    "\n",
    "ids=tf.constant([[2,3,4,5,6],[9,8,7,6,5]])\n",
    " \n",
    "start_logits= tf.constant([[0.5,0.1,0.1,0.1,0.1],[0.6,0.1,0.1,0.1,0.1]])\n",
    "end_logits= tf.constant([[0.1,0.1,0.1,0.1,0.7],[0.2,0.1,0.7,0.1,0.1]])\n",
    "\n",
    "get_best_span_prediction(ids,start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dnc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4f72099c4486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdnc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Ensure values are greater than epsilon to avoid numerical instability.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dnc'"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 Google Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"DNC addressing modules.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "from dnc import util\n",
    "\n",
    "# Ensure values are greater than epsilon to avoid numerical instability.\n",
    "_EPSILON = 1e-6\n",
    "\n",
    "TemporalLinkageState = collections.namedtuple('TemporalLinkageState',\n",
    "                                              ('link', 'precedence_weights'))\n",
    "\n",
    "\n",
    "def _vector_norms(m):\n",
    "  squared_norms = tf.reduce_sum(m * m, axis=2, keepdims=True)\n",
    "  return tf.sqrt(squared_norms + _EPSILON)\n",
    "\n",
    "\n",
    "def weighted_softmax(activations, strengths, strengths_op):\n",
    "  \"\"\"Returns softmax over activations multiplied by positive strengths.\n",
    "\n",
    "  Args:\n",
    "    activations: A tensor of shape `[batch_size, num_heads, memory_size]`, of\n",
    "      activations to be transformed. Softmax is taken over the last dimension.\n",
    "    strengths: A tensor of shape `[batch_size, num_heads]` containing strengths to\n",
    "      multiply by the activations prior to the softmax.\n",
    "    strengths_op: An operation to transform strengths before softmax.\n",
    "\n",
    "  Returns:\n",
    "    A tensor of same shape as `activations` with weighted softmax applied.\n",
    "  \"\"\"\n",
    "  transformed_strengths = tf.expand_dims(strengths_op(strengths), -1)\n",
    "  sharp_activations = activations * transformed_strengths\n",
    "  softmax = snt.BatchApply(module_or_op=tf.nn.softmax)\n",
    "  return softmax(sharp_activations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"DNC addressing modules.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "from dnc import util\n",
    "\n",
    "# Ensure values are greater than epsilon to avoid numerical instability.\n",
    "_EPSILON = 1e-6\n",
    "\n",
    "TemporalLinkageState = collections.namedtuple('TemporalLinkageState',\n",
    "                                              ('link', 'precedence_weights'))\n",
    "\n",
    "\n",
    "def _vector_norms(m):\n",
    "  squared_norms = tf.reduce_sum(m * m, axis=2, keepdims=True)\n",
    "  return tf.sqrt(squared_norms + _EPSILON)\n",
    "\n",
    "\n",
    "def weighted_softmax(activations, strengths, strengths_op):\n",
    "  \"\"\"Returns softmax over activations multiplied by positive strengths.\n",
    "\n",
    "  Args:\n",
    "    activations: A tensor of shape `[batch_size, num_heads, memory_size]`, of\n",
    "      activations to be transformed. Softmax is taken over the last dimension.\n",
    "    strengths: A tensor of shape `[batch_size, num_heads]` containing strengths to\n",
    "      multiply by the activations prior to the softmax.\n",
    "    strengths_op: An operation to transform strengths before softmax.\n",
    "\n",
    "  Returns:\n",
    "    A tensor of same shape as `activations` with weighted softmax applied.\n",
    "  \"\"\"\n",
    "  transformed_strengths = tf.expand_dims(strengths_op(strengths), -1)\n",
    "  sharp_activations = activations * transformed_strengths\n",
    "  softmax = snt.BatchApply(module_or_op=tf.nn.softmax)\n",
    "  return softmax(sharp_activations)\n",
    "\n",
    "\n",
    "class CosineWeights(snt.AbstractModule):\n",
    "  \"\"\"Cosine-weighted attention.\n",
    "\n",
    "  Calculates the cosine similarity between a query and each word in memory, then\n",
    "  applies a weighted softmax to return a sharp distribution.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads,\n",
    "               word_size,\n",
    "               strength_op=tf.nn.softplus,\n",
    "               name='cosine_weights'):\n",
    "    \"\"\"Initializes the CosineWeights module.\n",
    "\n",
    "    Args:\n",
    "      num_heads: number of memory heads.\n",
    "      word_size: memory word size.\n",
    "      strength_op: operation to apply to strengths (default is tf.nn.softplus).\n",
    "      name: module name (default 'cosine_weights')\n",
    "    \"\"\"\n",
    "    super(CosineWeights, self).__init__(name=name)\n",
    "    self._num_heads = num_heads\n",
    "    self._word_size = word_size\n",
    "    self._strength_op = strength_op\n",
    "\n",
    "  def _build(self, memory, keys, strengths):\n",
    "    \"\"\"Connects the CosineWeights module into the graph.\n",
    "\n",
    "    Args:\n",
    "      memory: A 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n",
    "      keys: A 3-D tensor of shape `[batch_size, num_heads, word_size]`.\n",
    "      strengths: A 2-D tensor of shape `[batch_size, num_heads]`.\n",
    "\n",
    "    Returns:\n",
    "      Weights tensor of shape `[batch_size, num_heads, memory_size]`.\n",
    "    \"\"\"\n",
    "    # Calculates the inner product between the query vector and words in memory.\n",
    "    dot = tf.matmul(keys, memory, adjoint_b=True)\n",
    "\n",
    "    # Outer product to compute denominator (euclidean norm of query and memory).\n",
    "    memory_norms = _vector_norms(memory)\n",
    "    key_norms = _vector_norms(keys)\n",
    "    norm = tf.matmul(key_norms, memory_norms, adjoint_b=True)\n",
    "\n",
    "    # Calculates cosine similarity between the query vector and words in memory.\n",
    "    similarity = dot / (norm + _EPSILON)\n",
    "\n",
    "    return weighted_softmax(similarity, strengths, self._strength_op)\n",
    "\n",
    "\n",
    "class TemporalLinkage(snt.RNNCore):\n",
    "  \"\"\"Keeps track of write order for forward and backward addressing.\n",
    "\n",
    "  This is a pseudo-RNNCore module, whose state is a pair `(link,\n",
    "  precedence_weights)`, where `link` is a (collection of) graphs for (possibly\n",
    "  multiple) write heads (represented by a tensor with values in the range\n",
    "  [0, 1]), and `precedence_weights` records the \"previous write locations\" used\n",
    "  to build the link graphs.\n",
    "\n",
    "  The function `directional_read_weights` computes addresses following the\n",
    "  forward and backward directions in the link graphs.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, memory_size, num_writes, name='temporal_linkage'):\n",
    "    \"\"\"Construct a TemporalLinkage module.\n",
    "\n",
    "    Args:\n",
    "      memory_size: The number of memory slots.\n",
    "      num_writes: The number of write heads.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super(TemporalLinkage, self).__init__(name=name)\n",
    "    self._memory_size = memory_size\n",
    "    self._num_writes = num_writes\n",
    "\n",
    "  def _build(self, write_weights, prev_state):\n",
    "    \"\"\"Calculate the updated linkage state given the write weights.\n",
    "\n",
    "    Args:\n",
    "      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          containing the memory addresses of the different write heads.\n",
    "      prev_state: `TemporalLinkageState` tuple containg a tensor `link` of\n",
    "          shape `[batch_size, num_writes, memory_size, memory_size]`, and a\n",
    "          tensor `precedence_weights` of shape `[batch_size, num_writes,\n",
    "          memory_size]` containing the aggregated history of recent writes.\n",
    "\n",
    "    Returns:\n",
    "      A `TemporalLinkageState` tuple `next_state`, which contains the updated\n",
    "      link and precedence weights.\n",
    "    \"\"\"\n",
    "    link = self._link(prev_state.link, prev_state.precedence_weights,\n",
    "                      write_weights)\n",
    "    precedence_weights = self._precedence_weights(prev_state.precedence_weights,\n",
    "                                                  write_weights)\n",
    "    return TemporalLinkageState(\n",
    "        link=link, precedence_weights=precedence_weights)\n",
    "\n",
    "  def directional_read_weights(self, link, prev_read_weights, forward):\n",
    "    \"\"\"Calculates the forward or the backward read weights.\n",
    "\n",
    "    For each read head (at a given address), there are `num_writes` link graphs\n",
    "    to follow. Thus this function computes a read address for each of the\n",
    "    `num_reads * num_writes` pairs of read and write heads.\n",
    "\n",
    "    Args:\n",
    "      link: tensor of shape `[batch_size, num_writes, memory_size,\n",
    "          memory_size]` representing the link graphs L_t.\n",
    "      prev_read_weights: tensor of shape `[batch_size, num_reads,\n",
    "          memory_size]` containing the previous read weights w_{t-1}^r.\n",
    "      forward: Boolean indicating whether to follow the \"future\" direction in\n",
    "          the link graph (True) or the \"past\" direction (False).\n",
    "\n",
    "    Returns:\n",
    "      tensor of shape `[batch_size, num_reads, num_writes, memory_size]`\n",
    "    \"\"\"\n",
    "    with tf.name_scope('directional_read_weights'):\n",
    "      # We calculate the forward and backward directions for each pair of\n",
    "      # read and write heads; hence we need to tile the read weights and do a\n",
    "      # sort of \"outer product\" to get this.\n",
    "      expanded_read_weights = tf.stack([prev_read_weights] * self._num_writes,\n",
    "                                       1)\n",
    "      result = tf.matmul(expanded_read_weights, link, adjoint_b=forward)\n",
    "      # Swap dimensions 1, 2 so order is [batch, reads, writes, memory]:\n",
    "      return tf.transpose(result, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def _link(self, prev_link, prev_precedence_weights, write_weights):\n",
    "    \"\"\"Calculates the new link graphs.\n",
    "\n",
    "    For each write head, the link is a directed graph (represented by a matrix\n",
    "    with entries in range [0, 1]) whose vertices are the memory locations, and\n",
    "    an edge indicates temporal ordering of writes.\n",
    "\n",
    "    Args:\n",
    "      prev_link: A tensor of shape `[batch_size, num_writes, memory_size,\n",
    "          memory_size]` representing the previous link graphs for each write\n",
    "          head.\n",
    "      prev_precedence_weights: A tensor of shape `[batch_size, num_writes,\n",
    "          memory_size]` which is the previous \"aggregated\" write weights for\n",
    "          each write head.\n",
    "      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          containing the new locations in memory written to.\n",
    "\n",
    "    Returns:\n",
    "      A tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n",
    "      containing the new link graphs for each write head.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('link'):\n",
    "      batch_size = tf.shape(prev_link)[0]\n",
    "      write_weights_i = tf.expand_dims(write_weights, 3)\n",
    "      write_weights_j = tf.expand_dims(write_weights, 2)\n",
    "      prev_precedence_weights_j = tf.expand_dims(prev_precedence_weights, 2)\n",
    "      prev_link_scale = 1 - write_weights_i - write_weights_j\n",
    "      new_link = write_weights_i * prev_precedence_weights_j\n",
    "      link = prev_link_scale * prev_link + new_link\n",
    "      # Return the link with the diagonal set to zero, to remove self-looping\n",
    "      # edges.\n",
    "      return tf.matrix_set_diag(\n",
    "          link,\n",
    "          tf.zeros(\n",
    "              [batch_size, self._num_writes, self._memory_size],\n",
    "              dtype=link.dtype))\n",
    "\n",
    "  def _precedence_weights(self, prev_precedence_weights, write_weights):\n",
    "    \"\"\"Calculates the new precedence weights given the current write weights.\n",
    "\n",
    "    The precedence weights are the \"aggregated write weights\" for each write\n",
    "    head, where write weights with sum close to zero will leave the precedence\n",
    "    weights unchanged, but with sum close to one will replace the precedence\n",
    "    weights.\n",
    "\n",
    "    Args:\n",
    "      prev_precedence_weights: A tensor of shape `[batch_size, num_writes,\n",
    "          memory_size]` containing the previous precedence weights.\n",
    "      write_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          containing the new write weights.\n",
    "\n",
    "    Returns:\n",
    "      A tensor of shape `[batch_size, num_writes, memory_size]` containing the\n",
    "      new precedence weights.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('precedence_weights'):\n",
    "      write_sum = tf.reduce_sum(write_weights, 2, keepdims=True)\n",
    "      return (1 - write_sum) * prev_precedence_weights + write_weights\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    \"\"\"Returns a `TemporalLinkageState` tuple of the state tensors' shapes.\"\"\"\n",
    "    return TemporalLinkageState(\n",
    "        link=tf.TensorShape(\n",
    "            [self._num_writes, self._memory_size, self._memory_size]),\n",
    "        precedence_weights=tf.TensorShape([self._num_writes,\n",
    "                                           self._memory_size]),)\n",
    "\n",
    "\n",
    "class Freeness(snt.RNNCore):\n",
    "  \"\"\"Memory usage that is increased by writing and decreased by reading.\n",
    "\n",
    "  This module is a pseudo-RNNCore whose state is a tensor with values in\n",
    "  the range [0, 1] indicating the usage of each of `memory_size` memory slots.\n",
    "\n",
    "  The usage is:\n",
    "\n",
    "  *   Increased by writing, where usage is increased towards 1 at the write\n",
    "      addresses.\n",
    "  *   Decreased by reading, where usage is decreased after reading from a\n",
    "      location when free_gate is close to 1.\n",
    "\n",
    "  The function `write_allocation_weights` can be invoked to get free locations\n",
    "  to write to for a number of write heads.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, memory_size, name='freeness'):\n",
    "    \"\"\"Creates a Freeness module.\n",
    "\n",
    "    Args:\n",
    "      memory_size: Number of memory slots.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super(Freeness, self).__init__(name=name)\n",
    "    self._memory_size = memory_size\n",
    "\n",
    "  def _build(self, write_weights, free_gate, read_weights, prev_usage):\n",
    "    \"\"\"Calculates the new memory usage u_t.\n",
    "\n",
    "    Memory that was written to in the previous time step will have its usage\n",
    "    increased; memory that was read from and the controller says can be \"freed\"\n",
    "    will have its usage decreased.\n",
    "\n",
    "    Args:\n",
    "      write_weights: tensor of shape `[batch_size, num_writes,\n",
    "          memory_size]` giving write weights at previous time step.\n",
    "      free_gate: tensor of shape `[batch_size, num_reads]` which indicates\n",
    "          which read heads read memory that can now be freed.\n",
    "      read_weights: tensor of shape `[batch_size, num_reads,\n",
    "          memory_size]` giving read weights at previous time step.\n",
    "      prev_usage: tensor of shape `[batch_size, memory_size]` giving\n",
    "          usage u_{t - 1} at the previous time step, with entries in range\n",
    "          [0, 1].\n",
    "\n",
    "    Returns:\n",
    "      tensor of shape `[batch_size, memory_size]` representing updated memory\n",
    "      usage.\n",
    "    \"\"\"\n",
    "    # Calculation of usage is not differentiable with respect to write weights.\n",
    "    write_weights = tf.stop_gradient(write_weights)\n",
    "    usage = self._usage_after_write(prev_usage, write_weights)\n",
    "    usage = self._usage_after_read(usage, free_gate, read_weights)\n",
    "    return usage\n",
    "\n",
    "  def write_allocation_weights(self, usage, write_gates, num_writes):\n",
    "    \"\"\"Calculates freeness-based locations for writing to.\n",
    "\n",
    "    This finds unused memory by ranking the memory locations by usage, for each\n",
    "    write head. (For more than one write head, we use a \"simulated new usage\"\n",
    "    which takes into account the fact that the previous write head will increase\n",
    "    the usage in that area of the memory.)\n",
    "\n",
    "    Args:\n",
    "      usage: A tensor of shape `[batch_size, memory_size]` representing\n",
    "          current memory usage.\n",
    "      write_gates: A tensor of shape `[batch_size, num_writes]` with values in\n",
    "          the range [0, 1] indicating how much each write head does writing\n",
    "          based on the address returned here (and hence how much usage\n",
    "          increases).\n",
    "      num_writes: The number of write heads to calculate write weights for.\n",
    "\n",
    "    Returns:\n",
    "      tensor of shape `[batch_size, num_writes, memory_size]` containing the\n",
    "          freeness-based write locations. Note that this isn't scaled by\n",
    "          `write_gate`; this scaling must be applied externally.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('write_allocation_weights'):\n",
    "      # expand gatings over memory locations\n",
    "      write_gates = tf.expand_dims(write_gates, -1)\n",
    "\n",
    "      allocation_weights = []\n",
    "      for i in range(num_writes):\n",
    "        allocation_weights.append(self._allocation(usage))\n",
    "        # update usage to take into account writing to this new allocation\n",
    "        usage += ((1 - usage) * write_gates[:, i, :] * allocation_weights[i])\n",
    "\n",
    "      # Pack the allocation weights for the write heads into one tensor.\n",
    "      return tf.stack(allocation_weights, axis=1)\n",
    "\n",
    "  def _usage_after_write(self, prev_usage, write_weights):\n",
    "    \"\"\"Calcualtes the new usage after writing to memory.\n",
    "\n",
    "    Args:\n",
    "      prev_usage: tensor of shape `[batch_size, memory_size]`.\n",
    "      write_weights: tensor of shape `[batch_size, num_writes, memory_size]`.\n",
    "\n",
    "    Returns:\n",
    "      New usage, a tensor of shape `[batch_size, memory_size]`.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('usage_after_write'):\n",
    "      # Calculate the aggregated effect of all write heads\n",
    "      write_weights = 1 - util.reduce_prod(1 - write_weights, 1)\n",
    "      return prev_usage + (1 - prev_usage) * write_weights\n",
    "\n",
    "  def _usage_after_read(self, prev_usage, free_gate, read_weights):\n",
    "    \"\"\"Calcualtes the new usage after reading and freeing from memory.\n",
    "\n",
    "    Args:\n",
    "      prev_usage: tensor of shape `[batch_size, memory_size]`.\n",
    "      free_gate: tensor of shape `[batch_size, num_reads]` with entries in the\n",
    "          range [0, 1] indicating the amount that locations read from can be\n",
    "          freed.\n",
    "      read_weights: tensor of shape `[batch_size, num_reads, memory_size]`.\n",
    "\n",
    "    Returns:\n",
    "      New usage, a tensor of shape `[batch_size, memory_size]`.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('usage_after_read'):\n",
    "      free_gate = tf.expand_dims(free_gate, -1)\n",
    "      free_read_weights = free_gate * read_weights\n",
    "      phi = util.reduce_prod(1 - free_read_weights, 1, name='phi')\n",
    "      return prev_usage * phi\n",
    "\n",
    "  def _allocation(self, usage):\n",
    "    r\"\"\"Computes allocation by sorting `usage`.\n",
    "\n",
    "    This corresponds to the value a = a_t[\\phi_t[j]] in the paper.\n",
    "\n",
    "    Args:\n",
    "      usage: tensor of shape `[batch_size, memory_size]` indicating current\n",
    "          memory usage. This is equal to u_t in the paper when we only have one\n",
    "          write head, but for multiple write heads, one should update the usage\n",
    "          while iterating through the write heads to take into account the\n",
    "          allocation returned by this function.\n",
    "\n",
    "    Returns:\n",
    "      Tensor of shape `[batch_size, memory_size]` corresponding to allocation.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('allocation'):\n",
    "      # Ensure values are not too small prior to cumprod.\n",
    "      usage = _EPSILON + (1 - _EPSILON) * usage\n",
    "\n",
    "      nonusage = 1 - usage\n",
    "      sorted_nonusage, indices = tf.nn.top_k(\n",
    "          nonusage, k=self._memory_size, name='sort')\n",
    "      sorted_usage = 1 - sorted_nonusage\n",
    "      prod_sorted_usage = tf.cumprod(sorted_usage, axis=1, exclusive=True)\n",
    "      sorted_allocation = sorted_nonusage * prod_sorted_usage\n",
    "      inverse_indices = util.batch_invert_permutation(indices)\n",
    "\n",
    "      # This final line \"unsorts\" sorted_allocation, so that the indexing\n",
    "      # corresponds to the original indexing of `usage`.\n",
    "      return util.batch_gather(sorted_allocation, inverse_indices)\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    \"\"\"Returns the shape of the state tensor.\"\"\"\n",
    "    return tf.TensorShape([self._memory_size])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
